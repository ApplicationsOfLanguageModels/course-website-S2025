---
layout: page
title: Week 2 - Pre-Training, Fine-Tuning, and Model Development
permalink: /lectures/week-2
parent: Lectures
nav_order: 2
---

## Topics Covered

### Pre-Training and Fine-Tuning of LLMs
This week covered foundational concepts in large language models (LLMs), specifically pre-training and fine-tuning. Discussions emphasized the need for large compute resources, high-quality data, and advanced architectures to achieve good performance on tasks like question answering and summarization. Techniques like parameter-efficient fine-tuning (PEFT) and prompt engineering were highlighted as essential for tailoring models to specific tasks.

### Challenges in Model Training
Key challenges in tokenization and model training were addressed, particularly how data modifications can be necessary when initial training results are unsatisfactory. Working memory's role in generating answers was also explored, along with strategies for model optimization and performance improvement.

## Key Discussions

### Data Tokenization and Model Performance
- Complexity of data tokenization and the need for adjustments during model training.
- Techniques for fine-tuning models with task-specific data.
- Importance of task objectives and structured approaches in model performance.

### Model Development and Reinforcement Learning
- Introduction to prompt-based learning and its role in generating accurate model responses.
- Use of supervised learning patterns and reinforcement learning to improve model consistency.

### Bhashaverse Project by Vandan Mujadia (PhD, LTRC, IIIT Hyderabad)
- Focus on developing translation models for 36 Indian languages.
- Challenges include script normalization, synthetic data augmentation for low-resource languages, and corpus creation.
- Strategies for improving machine translation quality through evaluation and error analysis.

## Resources

1. **Illustrated Transformer by Jay Alammar**  
   [read the blog here](https://jalammar.github.io/illustrated-transformer/)

2. **TechForward Research Seminar Series - January Edition**  
   [watch from 38:35 to 53:20](https://www.youtube.com/watch?v=gWOK7H0f_mg)

3. **LLMs Applications: An Introduction**  
   [view slides here](https://github.com/ApplicationsOfLanguageModels/course-website-S2025/blob/main/assets/%20slides/2025-01-06.pdf)

4. **Bhashaverse Project Slides**  
   [view slides here](https://github.com/ApplicationsOfLanguageModels/course-website-S2025/blob/main/assets/%20slides/Bhashaverse.pdf)

5. **Bhashaverse Research Paper**  
    [view paper here](https://arxiv.org/pdf/2412.04351)

6. **Stanford CS229 | Building Large Language Models (LLMs)**
    [view video here](https://www.youtube.com/watch?v=9vM4p9NN0Ts)

7. **Prompting Guide (detailed explanations and references)**
    [visit wesbite Here](https://www.youtube.com/watch?v=9vM4p9NN0Ts)

## Summary
The lectures covered technical aspects of pre-training and fine-tuning LLMs, focusing on the practical challenges in tokenization, data adjustments, and task-specific performance. Additionally, Bhashaverse was introduced as a significant project aimed at advancing machine translation for Indian languages. The importance of model optimization techniques like PEFT and prompt engineering was emphasized for adapting LLMs to real-world tasks.
